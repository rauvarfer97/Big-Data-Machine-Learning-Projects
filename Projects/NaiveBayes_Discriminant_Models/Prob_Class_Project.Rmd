---
title: "Comparative Analysis of Classification Models: Evaluating Naïve Bayes and Discriminant Analysis Approaches"
author: "Raúl Varela Ferrando"
always_allow_html: true
output: pdf_document
---

<div class=text-justify>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r paquetes, include=FALSE}
library(tidyverse)
library(tinytex)
library(MASS)
library(readxl)
library(e1071)
library(caret)
library(ROCR)
library(kernlab)
library(naivebayes)
library(pROC)
library(ggplot2)
```

Before starting, it would be convenient to point out that in this work we have chosen a dataset already created, in our case, we have chosen the dataset **score_test.xlsx**, which we have modified within RStudio to eliminate the variable *id* which only had an identifying reason for each observation.

First we loaded our data, standardizing the numerical variables so that they share mean and standard deviation.

```{r UploadData}
data0 = read_excel('score_test.xlsx')[,-1]
var.num = c(2:8)
data = as.data.frame(scale(data0[,var.num]))
data$Employee = as.factor(data0$Employee)
```

Once our data are loaded, we proceed to partition them into training set and test set.

```{r Partitioning}
set.seed(1735791)
ind_train = createDataPartition(data$Employee, p=0.75, list=FALSE)
data_train = data[ind_train,]
data_test = data[-ind_train,]
```

Next, we proceed to create the Naive-Bayes model with the “e1071” library.

```{r e1071}
modelNB0 = naiveBayes(x=subset(data_train, select = -Employee), 
                     y=data_train$Employee, laplace=0)
predNB0 = predict(modelNB0, newdata = data_test)
matconfNB0 = table(predNB0, data_test$Employee, dnn = c('Predicción','Real'))
matconfNB0

hitsNB0 = (matconfNB0[1,1]+matconfNB0[2,2])/sum(matconfNB0)
sensNB0 = matconfNB0[2,2]/sum(matconfNB0[,2])
specNB0 = matconfNB0[1,1]/sum(matconfNB0[,1])

cat(" Percentage Hit test = ",
    100*hitsNB0,"%\n",
    "Sensitivity test (Employee) = ",
    100*sensNB0,"%\n",
    "Specificity test (No Employee) = ",
    100*especNB0,"%\n")
```

As can be seen through the three variables created in the previous chunk, we obtain a sensitivity of 97.4%, a specificity of 99.3% and a percentage of hits in total of 97.7%, values that are quite acceptable. I would also like to point out that I have calculated these values manually and not with the help of the specific functions of the *caret* library since in this case the 1 would represent the *Employee* case and the 0, the *No Employee* case, so the confusion matrix would not be arranged correctly to calculate these values.

```{r e0710Grafics}
probabi0 = predict(modelNB0,data_test,
                   type="raw")[,2]
prediobj0 = prediction(probabi0,data_test$Employee)
plot(performance(prediobj0, "tpr","fpr"),
     main="COR TEST. Naive Bayes, Employee (Gauss)",
     xlab="False employee rate", 
     ylab="True employee rate")
abline(a=0,b=1,col="red",lty=2)
aucNB0<- as.numeric(performance(prediobj0,"auc")@y.values)
legend("bottomright",legend=paste("AUC=",round(aucNB0,3)))
```

```{r e0710Result}
Resul=c(Hits=100*hitsNB0,AUC=100*aucNB0, Sensitivity=100*sensNB0, Specificity=100*especNB0)
Resul
```

The values obtained for the percentage of hits, sensitivity and specificity are quite good, even the AUC value, which represents the area under the ROC curve, is very close to the optimum. However, we will study other models by changing the arguments of our *lda()* function. 

```{r naivebayeskernel}
modelNB1 = naive_bayes(x=subset(data_train, select = -Employee), y=data_train$Employee, laplace = 0, usekernel=TRUE)
predNB1 = predict(modelNB1, newdata = data_test)
matconfNB1 = table(predNB1, data_test$Employee, dnn = c('Predicción','Real'))
matconfNB1

hitsNB1 = (matconfNB1[1,1]+matconfNB1[2,2])/sum(matconfNB1)
sensNB1 = matconfNB1[2,2]/sum(matconfNB1[,2])
specNB1 = matconfNB1[1,1]/sum(matconfNB1[,1])

cat(" Percentage Hit test = ",
    100*hitsNB1,"%\n",
    "Sensitivity test (Employee) = ",
    100*sensNB1,"%\n",
    "Specificity test (No Employee) = ",
    100*specNB1,"%\n")
```

```{r naivebayeskernelGrafics}
probabi1<- predict(modelNB1,data_test,
                   type="prob")[,2]
prediobj1<-prediction(probabi1,data_test$Employee)
plot(performance(prediobj1, "tpr","fpr"),
     main="COR TEST. Naive Bayes, Employee (Kernel)",
     xlab="Fake hits rate", ylab="True hits rate")
abline(a=0,b=1,col="blue",lty=2)
aucNB1<- as.numeric(performance(prediobj1,"auc")@y.values)
legend("bottomright",legend=paste("AUC=",round(aucNB1,3)))
```

```{r naivebayeskernelResult}
Resul1=c(Hits=100*hitsNB1,AUC=100*aucNB1, Sensitivity=100*sensNB1, Specificity=100*especNB1)
Resul1
```

Through the kernel method we improve the AUC value, but we obtain a very poor prediction for the *No Employee* case, so we will continue to study other models.

```{r naivebayespoisson}
modelNB2 = naive_bayes(x=subset(data_train, select = -Employee), y=data_train$Employee, laplace = 0, usepoisson=TRUE)
predNB2 = predict(modelNB2, newdata = data_test)
matconfNB2 = table(predNB2, data_test$Employee, dnn = c('Predicción','Real'))
matconfNB2

hitsNB2 = (matconfNB2[1,1]+matconfNB2[2,2])/sum(matconfNB2)
sensNB2 = matconfNB2[2,2]/sum(matconfNB2[,2])
specNB2 = matconfNB2[1,1]/sum(matconfNB2[,1])

cat("Percentage Hit test = ",
    100*hitsNB2,"%\n",
    "Sensitivity test (Employee) = ",
    100*sensNB2,"%\n",
    "Specificity test (No Employee) = ",
    100*specNB2,"%\n")
```

```{r naivebayespoissonGrafics}
probabi2<- predict(modelNB2,data_test,
                   type="prob")[,2] 
prediobj2<-prediction(probabi2,data_test$Employee)
plot(performance(prediobj2, "tpr","fpr"),
     main="COR TEST. Naive Bayes, Employees (Poisson)",
     xlab="Fake hits rate", ylab="True hits rate")
abline(a=0,b=1,col="blue",lty=2)
aucNB2<- as.numeric(performance(prediobj2,"auc")@y.values)
legend("bottomright",legend=paste("AUC=",round(aucNB2,3)))

```
```{r naivebayespoissonResult}
Resul2=c(Hits=100*aciertosNB2,AUC=100*aucNB2,Sensitivity=100*sensNB2, Specificity=100*especNB2)
Resul2
```

```{r PruebaAUC}
roc0<- roc(data_test$Employee,probabi0)
roc1<- roc(data_test$Employee,probabi1)
roc2<- roc(data_test$Employee,probabi2)

plot(roc0,col="red",lwd=2,
     main="ROC test")
plot(roc1,add=TRUE,col="green",lwd=2)
plot(roc2,add=TRUE,col="blue",lwd=2)
legend("bottomright",col=c("red","green","blue"),
       lty=1,legend=c("NB Gauss","NB Kernel", "NB Poisson"))

roc.test(roc0,roc1)
roc.test(roc0,roc2)
roc.test(roc1,roc2)
```
The p-value obtained for the comparisons of the NB Gauss and NB Poisson models (roc0 and roc2), so we can say that the hypothesis that both models are indistinguishable is correct, although the small difference in values that we have in the AUC makes us opt for the NB Poisson model, while in the other cases, although we do not obtain p-values small enough, it is sufficient to consider the NB Gauss and NB Poisson models better than the NB Kernel.

Next, we will proceed to create our discriminant analysis models, which will be linear and quadratic.

```{r ADL}
modelADL = lda(data_train[,-8], data_train$Employee)

predictADL=predict(modelADL,newdata=data_test[,-8])$class

matconfADL=table('Prediction'=predictADL, 'Real'=data_test$Employee)
matconfADL

hitsLDA = (matconfADL[1,1]+matconfADL[2,2])/sum(matconfADL)
hitsLDA

hitsADL = (matconfADL[1,1]+matconfADL[2,2])/sum(matconfADL)
sensADL = matconfADL[2,2]/sum(matconfADL[,2])
specADL = matconfADL[1,1]/sum(matconfADL[,1])

cat(" Percentage Hit test = ",
    100*hitsADL,"%\n",
    "Sensitivity test (Employee) = ",
    100*sensADL,"%\n",
    "Specificity test (No Employee) = ",
    100*specADL,"%\n")
```

```{r ADLCV}
modelADLCV = lda(Employee ~ . , data = data, CV=TRUE)

predictADLCV = modelADLCV$class

matconfADLCV=table(predictADLCV, data$Employee, dnn=c('Clase real','Predicción'))

matconfADLCV

hitsADLCV = (matconfADLCV[1,1]+matconfADLCV[2,2])/sum(matconfADLCV)
sensADLCV = matconfADLCV[2,2]/sum(matconfADLCV[,2])
specADLCV = matconfADLCV[1,1]/sum(matconfADLCV[,1])

cat(" Percentage Hit test = ",
    100*hitsADLCV,"%\n",
    "Sensitivity test (Employee) = ",
    100*sensADLCV,"%\n",
    "Specificity test (No Employee) = ",
    100*specADLCV,"%\n")
```

As can be seen, both models are practically equivalent, although neither makes a good discrimination, since it predicts all values as 1, thus making a very bad classification. It is also convenient to comment that I have tried to create the quadratic discriminant analysis models, but it returns the error “rank deficiency in group 0”, which after informing me, comes to say that we do not have enough information in our dataset to create the model that we want, which is strange to me, but I have not been able to solve.

As conclusion we have that the best model created would be *modelNB2*, which is a Naive-Bayes model with Poisson distribution, since it gives us the best results.
