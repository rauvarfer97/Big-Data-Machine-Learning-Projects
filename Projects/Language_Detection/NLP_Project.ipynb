{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Process (NPL) Project\n",
    "\n",
    "### Author: Raúl Varela Ferrando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following work consists of applying language detection in texts, as well as the preparation, evaluation, and comparison of models. This function is of great interest since many additional tasks depend on the language, such as lexical analysis, for example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation of the working dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to use is available on the page https://www.statmt.org/europarl/, called *“European Parliament Proceedings Parallel Corpus 1996-2011”*, which is nothing more than the transcripts of the sessions of the European Parliament and their translations into the different languages of the European Union. In our case, we will use the transcripts in German, Spanish, French, English, Italian, and Polish.  \n",
    "\n",
    "Since this corpus was prepared for the training of translation systems, we have an aligned corpus for each language, along with the same corpus in English. Since in this last case we do not have a specific corpus, we will use the English corpus from which the Spanish corpus used derives (*\"europarl-v7-es-en.en\"*).  \n",
    "\n",
    "The first task is to construct the training and validation datasets in a balanced way. In the assignment statement, we are suggested to use a strategy that consists of using a random selection model so that when traversing the original file, for every 11 lines traversed, 10 are sent to the training file and 1 to the evaluation file randomly.  \n",
    "\n",
    "First, we define a function that allows us to read each of the files within the folder that contains all the corpora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(document):\n",
    "    with open(document, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Combine all lines into a single document\n",
    "    merged = ' '.join(lines)\n",
    "    \n",
    "    # Split the document into individual documents\n",
    "    documents = merged.split('\\n.')\n",
    "    \n",
    "    # Remove leading and trailing whitespace from each document\n",
    "    documents = [doc.strip() for doc in documents if doc.strip()]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "languages = {\n",
    "    \"german\": \"europarl-v7.de-en.de\",\n",
    "    \"english\": \"europarl-v7.es-en.en\",\n",
    "    \"spanish\": \"europarl-v7.es-en.es\",\n",
    "    \"french\": \"europarl-v7.fr-en.fr\",\n",
    "    \"italian\": \"europarl-v7.it-en.it\",\n",
    "    \"polish\": \"europarl-v7.pl-en.pl\"\n",
    "}\n",
    "\n",
    "# A dictionary to store the documents by language\n",
    "document_dict = {}\n",
    "\n",
    "for key, value in languages.items():\n",
    "    documents = read_documents(value)\n",
    "    document_dict[key] = documents\n",
    "\n",
    "# Now, document_dict contains the documents by language\n",
    "# You can access them using document_dict[\"german\"], document_dict[\"english\"], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dictionary containing the datasets by language has been created, we generate the training and validation datasets as described earlier, thus obtaining a dictionary for each set. Additionally, we have implemented a function to remove common punctuation marks from our text and normalize it to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re  # Import the regular expressions module\n",
    "\n",
    "# Define the proportion of lines for training and validation (10 to 1)\n",
    "train_ratio = 10\n",
    "validation_ratio = 1\n",
    "\n",
    "# Function to normalize and clean the text\n",
    "def clean_text(text):\n",
    "    # Remove common punctuation marks (except the apostrophe if relevant)\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "\n",
    "    # Normalize text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Dictionary to store training and validation documents by language\n",
    "train_data = {}\n",
    "validation_data = {}\n",
    "\n",
    "# Iterate over each language in document_dict\n",
    "for language, documents in document_dict.items():\n",
    "    # Initialize lists for training and validation documents\n",
    "    train_documents = []\n",
    "    validation_documents = []\n",
    "\n",
    "    # Randomly split lines into training and validation\n",
    "    for document in documents:\n",
    "        lines = document.split('\\n')\n",
    "        random.shuffle(lines)  # Shuffle the lines randomly\n",
    "        total_lines = 110000  # Line limit to use\n",
    "\n",
    "        # Calculate the number of lines for training and validation\n",
    "        num_train_lines = (total_lines // (train_ratio + validation_ratio)) * train_ratio\n",
    "        train_lines = lines[:num_train_lines]\n",
    "        validation_lines = lines[num_train_lines:total_lines]\n",
    "\n",
    "        # Join the lines back into documents and apply normalization\n",
    "        train_document = ' '.join([clean_text(line) for line in train_lines])\n",
    "        validation_document = ' '.join([clean_text(line) for line in validation_lines])\n",
    "\n",
    "        train_documents.append(train_document)\n",
    "        validation_documents.append(validation_document)\n",
    "\n",
    "    # Store the training and validation documents in the dictionaries\n",
    "    train_data[language] = train_documents\n",
    "    validation_data[language] = validation_documents\n",
    "\n",
    "# Now, train_data contains the normalized training documents, and validation_data contains the validation documents\n",
    "# You can access them using train_data[\"german\"], validation_data[\"german\"], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, we proceed to calculate the 100 most frequent words for each of the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idioma: german\n",
      "die: 103713\n",
      "der: 89201\n",
      "und: 69987\n",
      "in: 40850\n",
      "zu: 33089\n",
      "den: 30163\n",
      "wir: 26167\n",
      "ich: 25407\n",
      "das: 25076\n",
      "für: 24987\n",
      "von: 24372\n",
      "ist: 24084\n",
      "es: 20082\n",
      "dass: 19555\n",
      "nicht: 18956\n",
      "des: 18896\n",
      "auf: 18228\n",
      "eine: 17735\n",
      "werden: 16302\n",
      "im: 15721\n",
      "mit: 14780\n",
      "sie: 14280\n",
      "auch: 14168\n",
      "dem: 12602\n",
      "ein: 12015\n",
      "wird: 11281\n",
      "sich: 11048\n",
      "haben: 10526\n",
      "sind: 10308\n",
      "hat: 9148\n",
      "um: 9134\n",
      "wie: 9089\n",
      "europäischen: 9063\n",
      "als: 8597\n",
      "kommission: 8381\n",
      "über: 8361\n",
      "daß: 8291\n",
      "diese: 8274\n",
      "herr: 7919\n",
      "an: 7838\n",
      "zur: 7731\n",
      "bei: 6893\n",
      "einer: 6832\n",
      "union: 6564\n",
      "dieser: 6458\n",
      "uns: 6450\n",
      "wenn: 6390\n",
      "müssen: 6022\n",
      "einen: 6005\n",
      "möchte: 5725\n",
      "aber: 5567\n",
      "aus: 5308\n",
      "vor: 5254\n",
      "präsident: 5246\n",
      "noch: 5155\n",
      "können: 5104\n",
      "so: 5090\n",
      "nach: 5086\n",
      "nur: 5059\n",
      "diesem: 4942\n",
      "kann: 4922\n",
      "zum: 4760\n",
      "was: 4738\n",
      "europäische: 4695\n",
      "parlament: 4675\n",
      "bericht: 4552\n",
      "durch: 4541\n",
      "sein: 4495\n",
      "oder: 4457\n",
      "sehr: 4404\n",
      "mitgliedstaaten: 4316\n",
      "dies: 4175\n",
      "dieses: 3893\n",
      "einem: 3737\n",
      "ihre: 3647\n",
      "frau: 3642\n",
      "muss: 3638\n",
      "europa: 3342\n",
      "wurde: 3329\n",
      "alle: 3266\n",
      "er: 3237\n",
      "hier: 3174\n",
      "man: 3115\n",
      "mehr: 3106\n",
      "damit: 3003\n",
      "rat: 2917\n",
      "diesen: 2822\n",
      "unsere: 2813\n",
      "eu: 2767\n",
      "habe: 2762\n",
      "eines: 2728\n",
      "zwischen: 2722\n",
      "gibt: 2660\n",
      "da: 2645\n",
      "frage: 2642\n",
      "gegen: 2591\n",
      "mich: 2581\n",
      "keine: 2571\n",
      "heute: 2500\n",
      "ihnen: 2491\n",
      "\n",
      "\n",
      "Idioma: english\n",
      "the: 191343\n",
      "of: 90651\n",
      "to: 84144\n",
      "and: 71837\n",
      "in: 59319\n",
      "that: 45904\n",
      "a: 42343\n",
      "is: 42296\n",
      "for: 29126\n",
      "we: 28839\n",
      "i: 27609\n",
      "this: 27145\n",
      "on: 25687\n",
      "it: 23524\n",
      "be: 22452\n",
      "are: 18796\n",
      "not: 18721\n",
      "as: 18007\n",
      "have: 17992\n",
      "with: 16244\n",
      "which: 15462\n",
      "european: 14740\n",
      "will: 13158\n",
      "by: 13070\n",
      "has: 11657\n",
      "mr: 9838\n",
      "at: 9063\n",
      "commission: 8726\n",
      "an: 8628\n",
      "also: 8474\n",
      "would: 8366\n",
      "all: 8261\n",
      "should: 7958\n",
      "can: 7881\n",
      "our: 7823\n",
      "but: 7807\n",
      "from: 7562\n",
      "must: 7299\n",
      "president: 7050\n",
      "there: 6926\n",
      "been: 6743\n",
      "union: 6538\n",
      "you: 6531\n",
      "'s: 6340\n",
      "parliament: 6128\n",
      "states: 6006\n",
      "member: 5878\n",
      "more: 5568\n",
      "was: 5566\n",
      "report: 5454\n",
      "these: 5439\n",
      "or: 5416\n",
      "its: 5394\n",
      "they: 5360\n",
      "do: 5203\n",
      "their: 5183\n",
      "like: 5180\n",
      "council: 5073\n",
      "very: 5061\n",
      "what: 5010\n",
      "one: 4771\n",
      "if: 4752\n",
      "europe: 4700\n",
      "so: 4588\n",
      "countries: 4358\n",
      "eu: 4341\n",
      "no: 4218\n",
      "': 4202\n",
      "my: 4178\n",
      "us: 4117\n",
      "about: 4047\n",
      "other: 4036\n",
      "who: 3953\n",
      "need: 3948\n",
      "policy: 3864\n",
      "people: 3849\n",
      "only: 3834\n",
      "important: 3685\n",
      "time: 3628\n",
      "because: 3580\n",
      "new: 3577\n",
      "now: 3450\n",
      "such: 3256\n",
      "am: 3186\n",
      "rights: 3165\n",
      "up: 3123\n",
      "out: 3064\n",
      "therefore: 3056\n",
      "support: 3047\n",
      "make: 3035\n",
      "those: 3030\n",
      "however: 2998\n",
      "when: 2996\n",
      "take: 2989\n",
      "into: 2966\n",
      "between: 2899\n",
      "work: 2881\n",
      "economic: 2827\n",
      "some: 2804\n",
      "any: 2769\n",
      "\n",
      "\n",
      "Idioma: spanish\n",
      "de: 181560\n",
      "la: 123666\n",
      "que: 95726\n",
      "en: 78790\n",
      "el: 70646\n",
      "y: 67448\n",
      "a: 58392\n",
      "los: 54430\n",
      "las: 37429\n",
      "del: 30693\n",
      "se: 29579\n",
      "por: 27406\n",
      "no: 25318\n",
      "un: 25023\n",
      "una: 24748\n",
      "para: 23883\n",
      "es: 22628\n",
      "con: 21328\n",
      "al: 14734\n",
      "lo: 12666\n",
      "ha: 12266\n",
      "este: 11962\n",
      "como: 11662\n",
      "sobre: 11410\n",
      "comisión: 11396\n",
      "más: 10809\n",
      "su: 10339\n",
      "esta: 8665\n",
      "también: 8359\n",
      "europea: 8197\n",
      "señor: 8190\n",
      "unión: 7100\n",
      "presidente: 6601\n",
      "parlamento: 6471\n",
      "estados: 6084\n",
      "pero: 5772\n",
      "si: 5744\n",
      "han: 5581\n",
      "informe: 5380\n",
      "sus: 5331\n",
      "consejo: 5313\n",
      "europeo: 5269\n",
      "sin: 5096\n",
      "miembros: 5081\n",
      "o: 4990\n",
      "me: 4933\n",
      "países: 4837\n",
      "política: 4710\n",
      "europa: 4661\n",
      "son: 4464\n",
      "entre: 4452\n",
      "todos: 4388\n",
      "muy: 4245\n",
      "ya: 4142\n",
      "nos: 4067\n",
      "está: 4051\n",
      "todo: 4042\n",
      "esto: 3760\n",
      "ser: 3697\n",
      "tanto: 3683\n",
      "mi: 3619\n",
      "ue: 3490\n",
      "debe: 3424\n",
      "lugar: 3416\n",
      "derechos: 3387\n",
      "acuerdo: 3368\n",
      "importante: 3328\n",
      "hay: 3270\n",
      "hemos: 3223\n",
      "parte: 3210\n",
      "puede: 3134\n",
      "contra: 2969\n",
      "hecho: 2926\n",
      "porque: 2855\n",
      "propuesta: 2837\n",
      "desarrollo: 2830\n",
      "creo: 2816\n",
      "tiene: 2763\n",
      "ahora: 2667\n",
      "así: 2660\n",
      "hacer: 2651\n",
      "trabajo: 2643\n",
      "cuestión: 2637\n",
      "debate: 2627\n",
      "estos: 2610\n",
      "señora: 2599\n",
      "seguridad: 2583\n",
      "respecto: 2560\n",
      "ciudadanos: 2539\n",
      "cuando: 2522\n",
      "sr: 2419\n",
      "nuestra: 2408\n",
      "debemos: 2386\n",
      "he: 2386\n",
      "situación: 2375\n",
      "medidas: 2368\n",
      "vez: 2362\n",
      "forma: 2356\n",
      "desde: 2342\n",
      "decir: 2340\n",
      "\n",
      "\n",
      "Idioma: french\n",
      "de: 144713\n",
      "la: 94895\n",
      "et: 66778\n",
      "le: 62794\n",
      "les: 57014\n",
      "à: 55507\n",
      "des: 52255\n",
      "que: 44239\n",
      "en: 39076\n",
      "nous: 30868\n",
      "du: 27483\n",
      "dans: 25605\n",
      "pour: 24838\n",
      "qui: 22892\n",
      "une: 21333\n",
      "ce: 21042\n",
      "je: 20763\n",
      "un: 19970\n",
      "est: 18813\n",
      "sur: 17576\n",
      "pas: 17497\n",
      "au: 17349\n",
      "a: 15964\n",
      "il: 15604\n",
      "par: 14967\n",
      "ne: 14785\n",
      "plus: 11933\n",
      "commission: 11408\n",
      "cette: 10504\n",
      "': 10278\n",
      "aux: 9179\n",
      "sont: 8504\n",
      "européenne: 8456\n",
      "mais: 8190\n",
      "avec: 7686\n",
      "ces: 7061\n",
      "président: 7007\n",
      "monsieur: 6821\n",
      "pays: 6743\n",
      "se: 6624\n",
      "être: 6346\n",
      "parlement: 6284\n",
      "vous: 6136\n",
      "rapport: 6113\n",
      "comme: 6052\n",
      "été: 6017\n",
      "ont: 5899\n",
      "fait: 5770\n",
      "tout: 5713\n",
      "l'union: 5634\n",
      "si: 5463\n",
      "politique: 5455\n",
      "conseil: 5302\n",
      "membres: 5115\n",
      "européen: 5017\n",
      "c'est: 5002\n",
      "états: 4907\n",
      "également: 4809\n",
      "ou: 4754\n",
      "notre: 4753\n",
      "y: 4696\n",
      "aussi: 4607\n",
      "d'une: 4370\n",
      "faire: 4309\n",
      "qu'il: 4272\n",
      "cela: 4211\n",
      "m: 4203\n",
      "tous: 4200\n",
      "bien: 4100\n",
      "avons: 4016\n",
      "leur: 3997\n",
      "même: 3947\n",
      "son: 3900\n",
      "très: 3871\n",
      "question: 3857\n",
      "doit: 3853\n",
      "d'un: 3783\n",
      "devons: 3436\n",
      "entre: 3409\n",
      "l: 3326\n",
      "nos: 3323\n",
      "droits: 3216\n",
      "voudrais: 2948\n",
      "donc: 2872\n",
      "elle: 2817\n",
      "contre: 2796\n",
      "non: 2790\n",
      "développement: 2766\n",
      "sans: 2766\n",
      "proposition: 2714\n",
      "travail: 2711\n",
      "ainsi: 2697\n",
      "dont: 2682\n",
      "peut: 2664\n",
      "deux: 2650\n",
      "soit: 2628\n",
      "encore: 2601\n",
      "commissaire: 2507\n",
      "ses: 2463\n",
      "on: 2440\n",
      "\n",
      "\n",
      "Idioma: italian\n",
      "di: 100209\n",
      "e: 68921\n",
      "che: 62233\n",
      "la: 57615\n",
      "il: 47055\n",
      "in: 46513\n",
      "per: 39509\n",
      "a: 34760\n",
      "è: 28701\n",
      "del: 27360\n",
      "un: 26776\n",
      "non: 25664\n",
      "della: 25094\n",
      "i: 23412\n",
      "le: 22765\n",
      "una: 20330\n",
      "dei: 17116\n",
      "si: 16110\n",
      "con: 13892\n",
      "delle: 13003\n",
      "al: 12878\n",
      "sono: 12847\n",
      "commissione: 11662\n",
      "alla: 11157\n",
      "questo: 11081\n",
      "da: 10335\n",
      "ha: 10277\n",
      "nel: 10160\n",
      "gli: 10112\n",
      "': 10024\n",
      "più: 9726\n",
      "europea: 9570\n",
      "anche: 8802\n",
      "presidente: 8627\n",
      "come: 8587\n",
      "stati: 7637\n",
      "ma: 7340\n",
      "essere: 6916\n",
      "parlamento: 6725\n",
      "signor: 6473\n",
      "se: 6342\n",
      "questa: 6166\n",
      "cui: 6119\n",
      "relazione: 5989\n",
      "sia: 5945\n",
      "degli: 5765\n",
      "consiglio: 5586\n",
      "nella: 5581\n",
      "ad: 5483\n",
      "europeo: 5251\n",
      "ai: 5180\n",
      "membri: 5154\n",
      "paesi: 5090\n",
      "alle: 5012\n",
      "quanto: 4951\n",
      "stato: 4868\n",
      "politica: 4830\n",
      "o: 4676\n",
      "ci: 4645\n",
      "lo: 4612\n",
      "sulla: 4589\n",
      "tutti: 4544\n",
      "dal: 4532\n",
      "hanno: 4391\n",
      "su: 4145\n",
      "loro: 4140\n",
      "abbiamo: 4082\n",
      "mi: 4035\n",
      "tra: 4020\n",
      "modo: 3957\n",
      "parte: 3918\n",
      "sul: 3872\n",
      "tale: 3822\n",
      "lavoro: 3598\n",
      "dalla: 3584\n",
      "molto: 3553\n",
      "fatto: 3486\n",
      "perché: 3477\n",
      "dobbiamo: 3463\n",
      "diritti: 3401\n",
      "ed: 3393\n",
      "deve: 3356\n",
      "nei: 3286\n",
      "vorrei: 3204\n",
      "proposta: 3120\n",
      "ho: 3103\n",
      "solo: 3042\n",
      "ciò: 3032\n",
      "dell'unione: 2990\n",
      "commissario: 2973\n",
      "sviluppo: 2893\n",
      "cittadini: 2802\n",
      "può: 2786\n",
      "ancora: 2769\n",
      "importante: 2713\n",
      "l: 2689\n",
      "sua: 2680\n",
      "questione: 2628\n",
      "tutto: 2594\n",
      "sicurezza: 2580\n",
      "\n",
      "\n",
      "Idioma: polish\n",
      "w: 77226\n",
      "i: 54270\n",
      "na: 34845\n",
      "z: 30029\n",
      "się: 26963\n",
      "do: 25741\n",
      "że: 24978\n",
      "nie: 22308\n",
      "jest: 19601\n",
      "to: 17597\n",
      "o: 12318\n",
      "a: 10380\n",
      "dla: 8897\n",
      "jak: 8658\n",
      "tym: 8234\n",
      "za: 7963\n",
      "oraz: 7606\n",
      "które: 7320\n",
      "przez: 7080\n",
      "tego: 6919\n",
      "panie: 6817\n",
      "są: 5956\n",
      "ue: 5800\n",
      "co: 5548\n",
      "europejskiej: 5508\n",
      "aby: 5298\n",
      "również: 5073\n",
      "od: 5025\n",
      "ale: 4714\n",
      "przewodniczący: 4710\n",
      "tej: 4598\n",
      "sprawie: 4562\n",
      "po: 4476\n",
      "także: 4445\n",
      "unii: 4382\n",
      "pani: 4356\n",
      "komisji: 4316\n",
      "bardzo: 4192\n",
      "być: 4047\n",
      "czy: 4000\n",
      "ze: 3948\n",
      "będzie: 3926\n",
      "tak: 3877\n",
      "ich: 3672\n",
      "musimy: 3643\n",
      "ma: 3642\n",
      "jednak: 3532\n",
      "państwa: 3531\n",
      "tych: 3471\n",
      "może: 3323\n",
      "roku: 3317\n",
      "ponieważ: 3256\n",
      "komisja: 3254\n",
      "ten: 3215\n",
      "tylko: 3172\n",
      "by: 3134\n",
      "który: 3066\n",
      "państw: 3028\n",
      "celu: 3011\n",
      "należy: 2992\n",
      "dlatego: 2971\n",
      "jako: 2936\n",
      "praw: 2904\n",
      "już: 2842\n",
      "członkowskich: 2762\n",
      "te: 2725\n",
      "też: 2706\n",
      "chciałbym: 2584\n",
      "polityki: 2582\n",
      "pracy: 2576\n",
      "która: 2550\n",
      "europejskiego: 2526\n",
      "sposób: 2504\n",
      "pan: 2420\n",
      "wszystkich: 2412\n",
      "parlamentu: 2351\n",
      "zakresie: 2350\n",
      "rzecz: 2313\n",
      "uważam: 2300\n",
      "musi: 2277\n",
      "lub: 2240\n",
      "parlament: 2225\n",
      "europejska: 2188\n",
      "jej: 2171\n",
      "pod: 2153\n",
      "których: 2152\n",
      "jeśli: 2148\n",
      "prawa: 2116\n",
      "działania: 2090\n",
      "rady: 2022\n",
      "członkowskie: 2006\n",
      "nam: 1965\n",
      "nad: 1965\n",
      "sprawozdania: 1957\n",
      "bardziej: 1947\n",
      "europy: 1933\n",
      "rozwoju: 1919\n",
      "sprawozdanie: 1915\n",
      "między: 1910\n",
      "jego: 1901\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Modify train_data to be a list of documents\n",
    "train_data_list = [train_data[language] for language in languages]\n",
    "\n",
    "# Define a function to get the 100 most frequent words\n",
    "def get_top_words(train_documents):\n",
    "    # Combine all training documents into a single text\n",
    "    corpus = ' '.join(train_documents)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(corpus)\n",
    "\n",
    "    # Calculate the frequency of each word\n",
    "    fdist = FreqDist(words)\n",
    "\n",
    "    # Get the 100 most frequent words\n",
    "    top_words = fdist.most_common(100)\n",
    "\n",
    "    return top_words\n",
    "\n",
    "# List to store the 100 most frequent words per language\n",
    "top_words_by_language = []\n",
    "\n",
    "# Iterate through each language in train_data\n",
    "for train_documents in train_data_list:\n",
    "    top_words = get_top_words(train_documents)\n",
    "    top_words_by_language.append(top_words)\n",
    "\n",
    "# Print the 100 most frequent words per language\n",
    "for i, top_words in enumerate(top_words_by_language):\n",
    "    language = list(languages.keys())[i]\n",
    "    print(f\"Language: {language}\")\n",
    "    for word, frequency in top_words:\n",
    "        print(f\"{word}: {frequency}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation of a TF-IDF model for language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are asked to build a TF-IDF model. To implement a TF-IDF model for document classification in the 6 languages, we first need to calculate the TF-IDF matrix for the training documents in each language. Then, we save the TF-IDF matrix.\n",
    "\n",
    "Once we have the TF-IDF matrices, we move on to the model implementation. Since we are dealing with a classification problem, the model could be Naive-Bayes, SVC, or any of the models studied in this master's program applied to these cases. I have chosen to use an SVC due to the good performance this model provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM classifier for language detection\n",
    "language_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Create a dictionary that associates each language with a number (label)\n",
    "language_labels = {language: i for i, language in enumerate(languages.keys())}\n",
    "\n",
    "# Create lists to store training documents and labels\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Iterate over each language in train_data\n",
    "for language, train_documents in train_data.items():\n",
    "    # Add the normalized documents to X_train\n",
    "    X_train.extend(train_documents)\n",
    "    # Assign the corresponding label to each document\n",
    "    y_train.extend([language_labels[language]] * len(train_documents))\n",
    "\n",
    "# Now, X_train contains the training documents and y_train the corresponding labels.\n",
    "\n",
    "# Initialize the TF-IDF vectorizer with appropriate parameters\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the TF-IDF vectorizer to the training documents\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Save the dictionary with the TF-IDF matrices to a file\n",
    "with open('tfidf_matrices.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_tfidf, file)\n",
    "\n",
    "# Train the SVM classifier with the TF-IDF data and labels\n",
    "language_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open('language_classifier_model.pkl', 'wb') as file:\n",
    "    pickle.dump(language_classifier, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation of the language detection model using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our training and validation sets, we proceed to implement the model evaluation, which we have indexed to the training in the same code, which is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n",
      "\n",
      "Precisión Global: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Load the SVM classifier from the file\n",
    "with open('language_classifier_model.pkl', 'rb') as file:\n",
    "    language_classifier = pickle.load(file)\n",
    "\n",
    "# Create lists to store evaluation documents and labels\n",
    "X_eval = []\n",
    "y_eval = []\n",
    "\n",
    "# Iterate over each language in validation_data (as done in Section 1)\n",
    "for language, eval_documents in validation_data.items():\n",
    "    # Add the normalized documents to X_eval\n",
    "    X_eval.extend(eval_documents)\n",
    "    # Assign the corresponding label to each document\n",
    "    y_eval.extend([language_labels[language]] * len(eval_documents))\n",
    "\n",
    "# Transform the evaluation documents into TF-IDF representation using the loaded TF-IDF model\n",
    "X_eval_tfidf = tfidf_vectorizer.transform(X_eval)\n",
    "\n",
    "# Predict the languages of the evaluation documents\n",
    "y_pred = language_classifier.predict(X_eval_tfidf)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion = confusion_matrix(y_eval, y_pred)\n",
    "\n",
    "# Compute the overall model accuracy\n",
    "accuracy = accuracy_score(y_eval, y_pred)\n",
    "\n",
    "# Print the confusion matrix and overall accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"\\nOverall Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cleaning of the training and evaluation corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are suggested to clean the corpus of each language, since, as detailed in the statement of the work, there are some lines or words from one language that are not found in the corpus of that language. For example, in the Spanish corpus, there are words like \"the\" that do not belong to this language.\n",
    "\n",
    "The strategy suggested in the statement is to calculate the 100 most frequent words in each language, search for the lines that contain those words in other languages, and ignore these lines. In this case, we need to redo the code used in the first sections to adapt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Define the function to get the 100 most frequent words of a language\n",
    "def get_top_words(train_documents):\n",
    "    # Combine all training documents into a single text\n",
    "    corpus = ' '.join(train_documents)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = re.findall(r'\\b\\w+\\b', corpus)\n",
    "\n",
    "    # Calculate the frequency of each word\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        word = word.lower()  # Normalize to lowercase\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 0\n",
    "        word_freq[word] += 1\n",
    "\n",
    "    # Sort words by frequency in descending order\n",
    "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the 100 most frequent words\n",
    "    top_words = [word for word, _ in sorted_words[:100]]\n",
    "\n",
    "    return top_words\n",
    "\n",
    "# Directory where the original corpus files are stored and where the cleaned corpora will be saved\n",
    "clean_corpus_dir = \"clean_corpus\"\n",
    "\n",
    "# Create a directory for the cleaned corpora if it does not exist\n",
    "if not os.path.exists(clean_corpus_dir):\n",
    "    os.makedirs(clean_corpus_dir)\n",
    "\n",
    "# Iterate over each language in languages\n",
    "for language, filename in languages.items():\n",
    "    # Read the original training documents\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Get the 100 most frequent words of this language\n",
    "    top_words = get_top_words(train_data[language])\n",
    "\n",
    "    # Initialize lists for valid training and evaluation lines\n",
    "    valid_train_lines = []\n",
    "    valid_eval_lines = []\n",
    "\n",
    "    # Iterate over all lines in the original corpus\n",
    "    for line in lines:\n",
    "        # Tokenize the line into words\n",
    "        words = re.findall(r'\\b\\w+\\b', line)\n",
    "\n",
    "        # Check if any word is among the 100 most frequent\n",
    "        if any(word.lower() in top_words for word in words):\n",
    "            # Consider the line as valid\n",
    "            valid_train_lines.append(line)\n",
    "\n",
    "    # Take 100,000 lines for training and 10,000 lines for evaluation\n",
    "    num_train_lines = 100000\n",
    "    num_eval_lines = 10000\n",
    "    random.shuffle(valid_train_lines)\n",
    "    valid_eval_lines = valid_train_lines[:num_eval_lines]\n",
    "    valid_train_lines = valid_train_lines[num_eval_lines:num_eval_lines + num_train_lines]\n",
    "\n",
    "    # Save the valid lines in cleaned corpus files\n",
    "    clean_train_filename = os.path.join(clean_corpus_dir, f\"{language}_train.txt\")\n",
    "    clean_eval_filename = os.path.join(clean_corpus_dir, f\"{language}_eval.txt\")\n",
    "\n",
    "    with open(clean_train_filename, 'w', encoding='utf-8') as train_file:\n",
    "        train_file.writelines(valid_train_lines)\n",
    "\n",
    "    with open(clean_eval_filename, 'w', encoding='utf-8') as eval_file:\n",
    "        eval_file.writelines(valid_eval_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. TF-IDF with the clean corpus model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we must redo points 2 and 3 with the cleaned corpus, so that we can observe the impact of corpus cleaning on the model's performance.\n",
    "\n",
    "First, we read the files with the cleaned corpora and store them in dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Dictionary to store cleaned training documents by language\n",
    "clean_train_data = {}\n",
    "\n",
    "# Dictionary to store cleaned validation documents by language\n",
    "clean_validation_data = {}\n",
    "\n",
    "# Iterate through each language in languages\n",
    "for language in languages.keys():\n",
    "    # Read cleaned training documents\n",
    "    clean_train_filename = os.path.join(clean_corpus_dir, f\"{language}_train.txt\")\n",
    "    with open(clean_train_filename, 'r', encoding='utf-8') as file:\n",
    "        clean_train_documents = file.readlines()\n",
    "    \n",
    "    # Read cleaned validation documents\n",
    "    clean_eval_filename = os.path.join(clean_corpus_dir, f\"{language}_eval.txt\")\n",
    "    with open(clean_eval_filename, 'r', encoding='utf-8') as file:\n",
    "        clean_eval_documents = file.readlines()\n",
    "\n",
    "    # Store cleaned documents in dictionaries\n",
    "    clean_train_data[language] = clean_train_documents\n",
    "    clean_validation_data[language] = clean_eval_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the TF-IDF vectorizer, fit the cleaned training data, and train an SVC model with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier for Language Detection\n",
    "language_classifier_clean = SVC(kernel='linear')\n",
    "\n",
    "# Create lists to store training documents and labels\n",
    "X_train_clean = []\n",
    "y_train_clean = []\n",
    "\n",
    "# Iterate through each language in clean_train_data\n",
    "for language, clean_train_documents in clean_train_data.items():\n",
    "    # Add normalized documents to X_train\n",
    "    X_train_clean.extend(clean_train_documents)\n",
    "    # Assign the corresponding label to each document\n",
    "    y_train_clean.extend([language_labels[language]] * len(clean_train_documents))\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the TF-IDF vectorizer to the training documents\n",
    "X_train_clean_tfidf = tfidf_vectorizer.fit_transform(X_train_clean)\n",
    "\n",
    "# Save the dictionary with the TF-IDF matrices to a file\n",
    "with open('tfidf_matrices_clean.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_clean_tfidf, file)\n",
    "\n",
    "# Train the SVM classifier with TF-IDF data and labels\n",
    "language_classifier_clean.fit(X_train_clean_tfidf, y_train_clean)\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open('language_classifier_clean_model.pkl', 'wb') as file:\n",
    "    pickle.dump(language_classifier_clean, file)\n",
    "\n",
    "# Create lists to store clean evaluation documents and labels\n",
    "X_eval_clean = []\n",
    "y_eval_clean = []\n",
    "\n",
    "# Iterate through each language in clean_validation_data\n",
    "for language, clean_eval_documents in clean_validation_data.items():\n",
    "    # Add clean documents to X_eval_clean\n",
    "    X_eval_clean.extend(clean_eval_documents)\n",
    "    # Assign the corresponding label to each document\n",
    "    y_eval_clean.extend([language_labels[language]] * len(clean_eval_documents))\n",
    "\n",
    "# Transform the evaluation documents into TF-IDF representation using the TF-IDF model with clean data\n",
    "X_eval_clean_tfidf = tfidf_vectorizer.transform(X_eval_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión con Datos Limpios:\n",
      "[[9993    2    1    0    2    2]\n",
      " [   0 9991    4    2    2    1]\n",
      " [   0    3 9989    0    2    6]\n",
      " [   0    1    0 9994    3    2]\n",
      " [   0    0    2    0 9995    3]\n",
      " [   0    1    0    0    0 9999]]\n",
      "\n",
      "Precisión Global con Datos Limpios: 0.99935\n"
     ]
    }
   ],
   "source": [
    "# Predict the languages of the clean evaluation documents\n",
    "y_pred_clean = language_classifier_clean.predict(X_eval_clean_tfidf)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_clean = confusion_matrix(y_eval_clean, y_pred_clean)\n",
    "\n",
    "# Compute the overall accuracy of the model with clean data\n",
    "accuracy_clean = accuracy_score(y_eval_clean, y_pred_clean)\n",
    "\n",
    "# Print the confusion matrix and overall accuracy with clean data\n",
    "print(\"Confusion Matrix with Clean Data:\")\n",
    "print(confusion_clean)\n",
    "print(\"\\nOverall Accuracy with Clean Data:\", accuracy_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Language detection using n-grams or other language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the repetition of certain words in many languages, such as \"a\", which can be observed in various corpora, the use of **n-gramas** is recommended to detect word sequences of length **n**. In this case, we will use bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a dictionary to store bigram frequencies by language\n",
    "bigram_frequencies_by_language = {}\n",
    "\n",
    "# Iterate over each language in clean_train_data\n",
    "for language, clean_train_documents in clean_train_data.items():\n",
    "    # Initialize a bigram counter\n",
    "    bigram_counter = Counter()\n",
    "    \n",
    "    # Iterate over documents and count bigrams\n",
    "    for document in clean_train_documents:\n",
    "        words = document.split()  # Split the document into words\n",
    "        bigrams = zip(words, words[1:])  # Create bigrams\n",
    "        bigram_counter.update(bigrams)  # Update the bigram counter\n",
    "    \n",
    "    # Store bigram frequencies in the dictionary by language\n",
    "    bigram_frequencies_by_language[language] = bigram_counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have trained the bigram model, we will use it to evaluate language detection on the evaluation data. To do this, we need to calculate the probability that a sequence of bigrams belongs to a specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results by language with bigrams\n",
    "results_by_language_bigram = {}\n",
    "\n",
    "# Iterate through each language in bigram_frequencies_by_language\n",
    "for language, bigram_frequencies in bigram_frequencies_by_language.items():\n",
    "    # Calculate the total number of bigrams in the language\n",
    "    total_bigrams = sum(bigram_frequencies.values())\n",
    "    \n",
    "    # Calculate the probabilities of bigrams in the language\n",
    "    probabilities = {bigram: freq / total_bigrams for bigram, freq in bigram_frequencies.items()}\n",
    "    \n",
    "    # Initialize a list to store probabilities of evaluation documents\n",
    "    evaluation_probabilities = []\n",
    "    \n",
    "    # Iterate through evaluation documents\n",
    "    for document in clean_validation_data[language]:\n",
    "        words = document.split()  # Split the document into words\n",
    "        bigrams = zip(words, words[1:])  # Create bigrams\n",
    "        \n",
    "        # Calculate the probability of the bigram sequence\n",
    "        prob_sequence = 1.0\n",
    "        for bigram in bigrams:\n",
    "            prob_sequence *= probabilities.get(bigram, 0)  # Use 0 if the bigram does not exist in the language\n",
    "        \n",
    "        evaluation_probabilities.append(prob_sequence)\n",
    "    \n",
    "    # Predict the language with the highest probability\n",
    "    predicted_language = language  # Assume the current language is the most probable initially\n",
    "    max_probability = max(evaluation_probabilities)\n",
    "    for lang, prob in evaluation_probabilities.items():\n",
    "        if prob > max_probability:\n",
    "            predicted_language = lang\n",
    "            max_probability = prob\n",
    "    \n",
    "    # Store the results in the dictionary with bigrams\n",
    "    results_by_language_bigram[language] = {\n",
    "        'predicted_language': predicted_language,\n",
    "        'evaluation_probabilities': evaluation_probabilities\n",
    "    }\n",
    "\n",
    "# Print the results by language with bigrams\n",
    "for language, results in results_by_language_bigram.items():\n",
    "    print(f\"Language: {language}\")\n",
    "    print(f\"Predicted Language with Bigrams: {results['predicted_language']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
